---
title: Decision Trees, Bagging, Random Forests and Boosting
date: Nov 4, 2019
output: 
    html_document:
        theme: cosmo 
        toc: true
        toc_float: true
        highlight: tango
        number_sections: false
fig_width: 5
fig_height: 5
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center", 
                      out.width = '70%', 
                      cache = TRUE)
```

First, we load a few R packages
```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(magrittr)
library(rpart)
library(caret)
library(randomForest)
library(gbm)
library(here)
library(nycflights13)
library(rpart.plot)
```

**Attribution**: A lot of the material for this lecture came from the following resources

* [An Introduction to Statistical Learning, 2013](https://www.springer.com/us/book/9781461471370) by James, Witten, Hastie and Tibshirani
* [Slides on decision trees](https://github.com/datasciencelabs/2016/blob/master/lectures/ml/decision-trees.Rmd) by Rafael Irizarry 
* [Blogpost on decision trees](https://leightonzhang.com/2016/09/08/trees-and-forest/) by Leighton Zhang

# Motivation

In the last lecture, we described two types of 
machine learning algorithms: linear approaches, 
including linear regression, generalized linear models (GLM),
discriminant analysis, and model-free approaches (such
as $k$-nearest neighbors). The linear approaches were 
limited in that the partition of the prediction space 
had to be linear (or in the case of QDA, quadratic). 

Today, we look at a set powerful, popular, and well-studied 
methods that adapt to higher dimensions and also allow
these regions to take more complex shapes, and in some 
cases, still produce models that are interpretable.

We will focus on decision trees (including both regression and 
classification decision trees) and their extension to random 
forests.

## Decision trees 

Decision trees can be applied to both regression and
classification problems. We first consider regression 
problems, and then move on to classification.

### Motivating example 1

Let’s use a decision tree to decide what to 
eat for lunch!

Suppose the things that matter to you are 

1. the location of the restaurants and 
2. waiting time

What we would like to do is partition all the options for 
what to eat based on our ideal waiting time and money we have, 
and then predict how much it will cost.

The figure below shoes a decision tree. It consists of
splitting rules, starting at the top of tree and 
consists of the following components: 

* The tree grows from the root Whatever Food, which contains all possible food in the world.
* Segments of the tree are known as branches
* An internal node splits at some threshold, and two sides stand for two separated regions
* Leaves (or regions or terminal nodes) are final decisions. Multiple leaves may point to the same label.


```{r, echo=FALSE}
knitr::include_graphics("https://leightonzhang.files.wordpress.com/2016/09/what-to-eat-tree.png")
```
[[image source](https://leightonzhang.files.wordpress.com/2016/09/what-to-eat-tree.png)]

We can also convert the tree into different regions
for classification:

```{r, echo=FALSE}
knitr::include_graphics("https://leightonzhang.files.wordpress.com/2016/09/classification.png")
```
[[image source](https://leightonzhang.files.wordpress.com/2016/09/classification.png)]

The regions are 

* $R_1 = \{X | \texttt{ wait } < 5, \texttt{ distance } < 100\}$ (Rice)
* $R_2 = \{X | \texttt{ wait } < 15, \texttt{ distance } > 100\}$ (Steak)
* $R_3 = \{X | \texttt{ wait } > 5, \texttt{ distance } < 100\}$ (Noodles)
* $R_4 = \{X | \texttt{ wait } > 15, \texttt{ distance } > 100\}$ (Burger)

And for regression decision trees, they operate 
by predicting an outcome variable $Y$ by 
partitioning feature  (predictor) space. So here 
we will consider another dimension (cost in this case):

```{r, echo=FALSE}
knitr::include_graphics("https://leightonzhang.files.wordpress.com/2016/09/regression.png")
```
[[image source](https://leightonzhang.files.wordpress.com/2016/09/regression.png)]

The predicted cost for those restaurants is the 
mean cost for the restaurants in the individual 
regions. 

### Motivating example 2

Consider the following dataset containing information on 
572 different Italian olive oils from multiple regions 
in Italy. 

```{r}
olives <- read_csv(here("data", "olives.csv"), 
                   col_types = "fddddddddd")
region_names <- c("Southern Italy","   Sardinia","Northern Italy")
olives %<>% mutate(Area = as.factor(region_names[Area]))
olives
```

We are interested in building a classification tree where 
`Area` is the outcome variable. How many areas are there? 

```{r}
table(olives$Area)
```

OK there are three areas. 

Let's just consider two measured predictors: `linoleic` 
and `eicosenoic`. Suppose we wanted to predict the olive 
oil's area using these two predictors. What method would you use?

```{r}
p <- olives %>% 
  ggplot(aes(eicosenoic, linoleic, fill=Area)) +
  geom_point(pch=21)
p
```
Note that we can describe a classification algorithm
using only these two predictors that would work 
pretty much perfectly:

```{r}
p <- p + geom_vline(xintercept = 6.5) + 
  geom_segment(x= -2, y = 1053.5, xend = 6.5, yend = 1053.5)
p
```

The prediction algorithm inferred from the figure 
above is is what we call a _decision tree_. If `eicosnoic`
is larger than 6.5, predict Southern Italy. If not, then 
if `linoleic` is larger than $1053.5$ predict Sardinia 
and Norther Italy otherwise. 

We can draw this decision tree like this:

```{r, echo=FALSE}
fit <- rpart(Area~., 
             data = select(olives, Area, linoleic, eicosenoic))
plot(fit)
text(fit, cex = 0.5)
```

In the figure above we used the `rpart()` function in the 
`rpart` R package which stands for ``Recursive 
Partitioning and Regression Trees''. We'll learn
more about what that means in a bit. 

### Regression Trees

Let's start with case of a continuous outcome. 
The general idea here is to build a decision 
tree and at end of each _node_ we will have 
a different prediction $\hat{Y}$ for the 
outcome $Y$.

The regression tree model does the following:

1. Divide or partition the predictor space (that is the possible values for $X_1$, $X_2$, ... $X_p$) into $J$ distinct and non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, we make the same predition, which is simply the mean of the response values for training observations in $R_j$.

#### How to construct regions? 

In theory, the regions could have any shape. However,
we choose to divide the predictor space into high-dimensional 
rectangles, or boxes, for simplicity and for ease of 
interpretation of the resulting predictive model. 
The goal is to find boxes $R_1$, ... , $R_J$ that minimize: 

$$ RSS = \sum_{j=1}^J \sum_{i \in R_j} (y_i - \hat{y}_{R_j})^2 $$ 
where $\hat{y}_{R_j}$ is the mean response for the training 
observations within the $j^{th}$ box. 

This is a very computationally intenseive because we have
to consider every possible partition of the feature space
into $J$ boxes. 

Intead, we do a _top-down, greedy_ approach known as 
_recursive binary splitting_. The 'top-down' approach
successively splits the predictor space and the 'greedy' 
approach means at each step it looks for the _best_ split 
made at a particular step, rather than looking ahead and 
picking a split that will lead to a better tree in some 
future step. 

For example, consider finding a good predictor 
$j$ to partition space its axis. A recursive 
algorithm would look like this:

1. First select the predictor $X_j$ and cutpoint $s$ such that the splitting the predictor space into the regions $R_1(j,s) = \{X | X_j < s\}$ (aka the region of predictor space in which $X_j$ takes on a value less than $s$) and $R_2(j,s) = \{X | X_j \geq s \}$ (aka the region of predictor space in which $X_j$ takes on a value greater than or equal to $s$) leads to the greatest possible reduction in the residual sum of squares (RSS) or minimizes this: 

$$ \sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
   \sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2 $$

where $\hat{y}_{R_1}$ and $\hat{y}_{R_2}$ are the mean 
response for training observations in $R_1(j,s)$ and 
$R_2(j,s)$. 

Finding values of $j$ and $s$ that minimize the above can be 
done quickly, especially when the number of features $p$ is
not too large. 

2. Next, we repeat the process, looking for the best predictor
and best cutpoint in order to split the data further so as 
to minimize the RSS within each of the resulting regions. 

However, this time, instead of splitting the entire predictor 
space, we split one of the two previously identified regions. 
We now have three regions. Again, we look to split one of 
these three regions further, so as to minimize the RSS. 

3. The process continues until a stopping criterion is reached; 
for instance, we may continue until no region contains more 
than five observations.


#### Predicting the response 

Once the regions $R_1$,...,$R_J$ have been created, 
we predict the response for a given test observation using 
the mean of the training observations in the region to 
which that test observation belongs.

#### Tree pruning 

To avoid overfitting the data (meaning poor test set performance
because you have a very complex tree), a smaller tree with
fewer splits (meaning fewer regions) might lead to lower 
variance and better interpretation (at the cost of slightly 
more bias). 

A common solution to this is to grow a very large tree 
$T_0$ and then _prune_ it back to a _subtree_. Given a 
subtree, we can estimate its test error using cross-validation. 

Instead of considering every subtree, we use something called 
_cost complexity pruning_ or _weakest link pruning_ with a
nonnegative tuning parameter $\alpha$. 
You can read more about 
[Algorithm 8.1](http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf) on page 309. 

For a brief summary of the cost complexity pruning, we borrow 
an idea (similar to using the lasso to control the complexity 
of a linear model) for controling the complexity of a tree: 

For each value of $\alpha$ there corresponds a subtree $T \subset T_0$ such that

$$ \sum_{m=1}^{|T|} \sum_{x_i \in R_m} (y_i - \hat{y}_{R_m})^2 + \alpha |T| $$ 

where $|T|$ represents the number of terminal nodes of the tree $T$, 
$R_m$ is the rectangle (i.e. subset of the predictor space) corresponding 
to the $m^{th}$ terminal node and $\hat{y}_{R_m} is the predicted response 
associated with $R_m$ -- aka the mean of the training observations in $R_m$. 

The idea is that the tuning parameter $\alpha$ 
controls a trade-off between the subtree's complexity and 
its fit to the training data. When $\alpha = 0$, then the 
subtree $T$ will simply equal the original tree $T_0$, because then the above 
quanityt just measures the training error.  

However, as $\alpha$ increases, there is a price to pay for 
having a tree with many terminal nodes, so the quantity above will tend to be
minimized for a smaller subtree. Hence branches get
pruned from the tree in a nested and predictable fashion. 

We can select a value of $\alpha$ using a validation set or using 
cross-validation. We then return to the full data set and obtain
the subtree corresponding to $\alpha$. This process is summarized 
in Algorithm 8.1.


### Classification trees

A _classification tree_ is very similar to a _regression tree_,
except that it is used to predict a qualitative response rather 
than a quantitative one. Recall that for a regression tree, 
the predicted response for an observation is given by the mean
response of the training observations that belong to the same 
terminal node. 

In contrast, for a classification tree, we predict that each 
observation belongs to the _most commonly occurring class_ of 
training observations in the region to which it belongs. 
In interpreting the results of a classification tree, we are
often interested not only in the class prediction corresponding 
to a particular terminal node region, but also in the class 
proportions among the training observations that fall into 
that region.

We also use _recursive binary splitting_ to grow a classification
tree, but we cannot use $RSS$ as the criterion for making the binary
splits. A natural alternative to $RSS$ is the 
_classification error rate_. We assign an observation in a given region 
to the most commonly occurring class of training observations 
in that region. Then, the _classification error rate_ is simply 
the fraction of the training observations in that region that 
do not belong to the most common class:

$$ E = 1 - \max (\hat{p}_{mk}) $$ 

where $\hat{p}_{mk}$ represents the proportion of training 
observations in the $m^{th}$ region that are from the $k^{th}$
class. However, it turns out that classification error is not
sufficiently sensitive for tree-growing, and in practice 
two other measures are preferable.

1. The Gini index is defined by 

$$ G = \sum_{k=1}^K \hat{p}_{mk} * (1 - \hat{p}_{mk} ) $$ 

and is a measure of total variance across the $K$ classes. 
It is not hard to see that the Gini index takes on a small
value if all of the $\hat{p}_{mk}$s are close to zero or one.
For this reason the Gini index is referred to as a measure
of node _purity_ (a small value indicates that a node contains 
predominantly observations from a single class).

2. An alternative to the Gini index is cross-entropy, given by

$$ D = - \sum_{k=1}^K \hat{p}_{mk} \log (\hat{p}_{mk} ) $$

Since $0 \leq \hat{p}_{mk} \leq 1$, it follows that $0 \leq −\hat{p}_{mk} log(\hat{p}_{mk})$

Like the Gini index, the cross-entropy will take 
on a small value if the $m^{th}$ node is pure (aka if 
$\hat{p}_{mk}$s are close to zero or one). In fact, it 
turns out that the Gini index and the cross-entropy 
are quite similar numerically.

When building a classification tree, either the Gini 
index or the cross-entropy are typically used to evaluate
the _quality of a particular split_ (since these two 
approaches are more sensitive to node purity than is
the classification error rate). Any of these three 
approaches might be used when _pruning_ the tree, but the 
classification error rate is preferable if prediction 
accuracy of the final pruned tree is the goal.


# What is the data? 

In this lecture, we are going to build classification algorithms to predict whether or not domestic flights will arrive late to their destinations. To do this, we will use data that come from the [`hadley/nycflights13`](https://github.com/hadley/nycflights13) github repo. 

> "This package contains information about all flights that departed from NYC (e.g. EWR, JFK and LGA) to destinations in the United States, Puerto Rico, and the American Virgin Islands) in 2013: 336,776 flights in total. To help understand what causes delays, it also includes a number of other useful datasets."

This package provides the following data tables.

- `flights`: all flights that departed from NYC in 2013
- `weather`: hourly meterological data for each airport
- `planes`: construction information about each plane
- `airports`: airport names and locations
- `airlines`: translation between two letter carrier codes and names

# Data import 

To load the data, it is very straight forward. 

```{r}
library(nycflights13)
```

We can peek at what is in each data, by printing it: 

```{r}
flights
```

```{r}
airlines
```



# Data wrangling

Next, let's explore what are the column names inside each of these datasets. 

```{r}
list(flights = colnames(flights), 
     airlines = colnames(airlines), 
     weather = colnames(weather),
     airports = colnames(airports),
     planes = colnames(planes))
```

We see that some of the column names overlap. For example, the column name `carrier` exists in both `flights` and `airlines`. It would be nice to have the full name 
of the carrier instead of just the abbreviation. 

To do this, we can use the `join` functions from the `dplyr` package. For example, to the `flights` and `airlines` dataset, we can use the `left_join()` function: 

```{r}
flights %>%
  left_join(airlines, by = "carrier") %>% 
  select(arr_delay, carrier, name)
```

Now let's combine 4 of these datasets together. Note, in each case, I'm carefully specifying what to join each dataset by. 

```{r}
flights_all <- flights %>%
  left_join(airlines, by = "carrier") %>% 
  left_join(weather, by = c("year", "month", "day", "hour", "origin")) %>% 
  left_join(planes, by = "tailnum") # ignoring planes$year (year manufactured)
flights_all
```



# Exploratory Data Analysis

The column we are interested in is the `arr_delay` (arrival delays in minutes) where the negative times represent early arrivals. 

What would some variables that you think would be influential on whether or not a plane has a delayed arrival? 

One thing might be whether or not it had a delayed departure. Let's create a plot to see that relationship. 

```{r}
flights_all %>% 
  ggplot(aes(x=dep_delay, y=arr_delay, color = name)) + 
  geom_point()
```

Yup, that is strongly related. 

Ok, how about airlines carriers. Are there certain airlines that are have more delayed arrivals (on average) compared to other airlines? 

```{r}
flights_all %>% 
  mutate(name = forcats::fct_reorder(name, arr_delay, .fun = median, na.rm=TRUE)) %>% 
  ggplot(aes(x=name, y = arr_delay)) + 
  theme(axis.text.x = element_text(angle = 90)) + 
  coord_flip() + 
  geom_boxplot()
```

Possibily. 

```{r}
flights_all %>% 
  group_by(name) %>% 
  summarize(med_arr_delay = median(arr_delay, na.rm = TRUE), n()) %>% 
  arrange(desc(med_arr_delay))
```

What about which of the three airports that the flight originated from? 

```{r}
flights_all %>% 
  group_by(origin) %>% 
  summarize(median(arr_delay, na.rm = TRUE))
```


What about the size of the plane? A surrogate variable we could explore is the number of seats on a plane as proxy for the size. 

```{r}
flights_all %>% 
  ggplot(aes(x=seats, y=arr_delay, color = name)) + 
  geom_point() 
```

What about the hour of the day that the flight leaves? 

```{r}
flights_all %>% 
  ggplot(aes(x=hour, y=arr_delay, color = name)) + 
  geom_point() 
```

OK, so let's create a new column titled `arr_delay_status` that represents whether or not the plane arrived more than 15 mins late to its destination.  We will also select a subset of variables to consider for purposes of this lecutre. Finally, we drop any rows with `NA` and downsample to only 5,000 rows to keep the computational side small for the lecture. 

```{r}
set.seed(1234)
flights_all_clean <- flights_all %>% 
  mutate(arr_delay_status = factor(ifelse(arr_delay > 15, 1, 0))) %>% 
  select(arr_delay_status, dep_delay, name, seats, hour, origin) %>%
  drop_na() %>% 
  sample_n(size=5e3)

dim(flights_all_clean)
```

We can also explore whether or not we have a balanced dataset (i.e. we might expect that we have more `0`s vs `1`s, otherwise that would be really bad for airlines....)
```{r}
table(flights_all_clean$arr_delay_status)
```

# Data analysis 


## Split data into train/tune/test

We will split the data into a training and testing using the `createDataPartition()` function in the [caret package](http://topepo.github.io/caret/index.html) with the argument `p` being the percentages of data that goes into training: 

```{r}
set.seed(1234)
train_set = createDataPartition(y = flights_all_clean$arr_delay_status, 
                                p = 0.8, list=FALSE)

train_dat = flights_all_clean[train_set,]
test_dat = flights_all_clean[-train_set,]
```


## Classification trees using `rpart`

To build a classification tree, we will use the `train()` function with the 
`method = "rpart"` argument from the `caret` package. We briefly saw this function
in our introduction to machine learning lecture. Now you know a bit more about 
what this means. 

```{r}
rpartfit <- train(arr_delay_status ~ ., 
                  method = "rpart", data = train_dat)
rpartfit
```

We can see how are we are doing in our training error with the `confusionMatrix()` function. 

```{r}
newdata <- as.data.frame(select(train_dat, -arr_delay_status))
pred_rpart <- predict(rpartfit, newdata)
confusionMatrix(train_dat$arr_delay_status, pred_rpart)
```


**Note**: Kappa (or Cohen’s Kappa) is like classification accuracy, except that it is normalized at the baseline of random chance on your dataset. It is a more useful measure to use on problems that have an imbalance in the classes (e.g. 70-30 split for classes 0 and 1 and you can achieve 70% accuracy by predicting all instances are for class 0). 

We can plot the model using the `rpart.plot()` function. 

```{r}
rpart.plot(rpartfit$finalModel)
```

Each node shows: 

- the predicted of a delayed arrival (by 15 mins) or not,
- the predicted probability of a delayed arrival,
- the percentage of observations in the node.

Now, if you look closely above, you will see that there is some tuning going on. 
We haven't talked about this yet, but if you are using the `caret` package with `method = "rpart"`, this is pruning the tree. The pruning is happening using a complexity parameter (`cp`). This that $\alpha$ tuning parameter that we talked about above. If you do not want to use the algorithm, you can control this parameter using the `tuneGrid` argument in `train()`. 

```{r}
train(arr_delay_status ~ ., 
      method = "rpart", data = train_dat, 
      tuneGrid = expand.grid(cp = seq(0, .6, by = .2)))
```

Next, we set up the parameters using the `trainControl()` function  
in the `caret` package to provide more details on how to train
the algorithm in `train()`. The default is to use the bootstrap 
and here `number` refers to the number or resampling iterations. 

```{r}
fitControl <- trainControl(method="boot", number=10)

train(arr_delay_status ~ ., 
      method = "rpart", data = train_dat,
      trControl = fitControl)
```

Alternatively, we can ask for `number=5` cross-fold in a cross-fold 
validation for tuning our complexity parameter.  

```{r}
fitControl <- trainControl(method="cv", number=5)

train(arr_delay_status ~ ., 
      method = "rpart", data = train_dat,
      trControl = fitControl)
```

You can also try `method="repeatedcv", number=5, repeats=3` in our cross validation 
and ask to repeat that three times (`repeat=3`).

```{r, eval=FALSE}
fitControl <- trainControl(method="repeatedcv", number=5, repeats=3)
```


## Classification using `glm`

We can also compare to how the regression trees compare to something like 
logistic regression that we learned last time. 

```{r, warning=FALSE, message=FALSE}
fitControl <- trainControl(method = "cv", number = 5)

glmfit <- train(arr_delay_status ~ ., 
                method = "glm", data = train_dat, 
                trControl = fitControl)
glmfit
```

```{r}
newdata <- as.data.frame(select(train_dat, -arr_delay_status))
pred_glm <- predict(glmfit, newdata)
confusionMatrix(train_dat$arr_delay_status, pred_glm)

```


### More details on decision trees 

Why use decision trees? 

Decision trees for regression and classification have 
a number of advantages over the more classical 
classification approaches.

#### Advantages 

1. Trees are very easy to explain to people. In fact, they are even easier to explain than linear regression!
2. Some people believe that decision trees more closely mirror human decision-making than do the regression and classification approaches seen in previous lectures.
3. Trees can be displayed graphically, and are easily interpreted even by a non-expert (especially if they are small).
4. Trees can easily handle qualitative predictors without the need to create dummy variables.

#### Disadvantages

1. Trees generally do not have the same level of predictive accuracy as some of the other regression and classification approaches .

However, by aggregating many decision trees, using 
methods like _bagging_, _random forests_, and _boosting_, 
the predictive performance of trees can be substantially 
improved. We introduce these concepts next.

## Bagging 

Bootstrap aggregation (or _bagging_) is a general-purpose 
technique used to improve the variance of a statistical 
learning method. Here, we will use it to improve the 
performance of decision trees, which suffers from 
high variance. Meaning if we split the training data into
two parts at random, and fit a decision tree to both halves, 
the results that we get could be quite different.

In general, to reduce the variance, one approach is to take
many training sets from the population, build a separate 
prediction model (e.g. a decision tree) using each training set, 
and _average_ the resulting predictions (e.g. majority vote). 
In other words, we could calculate $\hat{f}^{1}(x)$, $\hat{f}^2(x)$,
..., $\hat{f}^B(x)$ using $B$ separate training sets, and
average them in order to obtain a single low-variance
statistical learning model, given by

$$ \hat{f}_{avg}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^b(x) $$

Of course, this is not practical because we generally do not 
have access to multiple training sets. 

The key idea here is to use [_boostrap samples_](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) (or random sampling with replacement) from the (single)
training data set. We generate $B$ different bootstrapped 
training datasets, train our method on the $b^{th}$ bootstrapped 
training set in order to get $\hat{f}^{∗b}(x)$, and finally 
average all the predictions, to obtain

$$ \hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{*b}(x) $$

This is called _bagging_. 

### Bagging with regression trees 

To apply bagging to regression trees with a 
quantitative outcome $Y$ : 

1. Construct $B$ trees using $B$ bootstrapped training sets (trees should be deep and not pruned)
2. Average the resulting predictions 

Hence each individual tree has high variance, but low bias. 
Averaging these $B$ trees reduces the variance. 

Bagging has been demonstrated to give impressive improvements 
in accuracy by combining together hundreds or even thousands 
of trees into a single procedure.


### Bagging with classification trees 

To apply bagging to classification trees with a 
qualitative outcome $Y$: 

Bagging be extended to a classification problem using 
a few possible approaches, but the simplest is as follows. 

1. For a given test observation, we can record the class 
predicted by each of the $B$ trees
2. Average the resulting predictions by taking a majority vote (the overall prediction is the most commonly occurring class among the B predictions)

The number of trees $B$ is not a critical parameter with bagging. 
Using a very large value of $B$ will not lead to overfitting. 
In practice we use a value of $B$ sufficiently large that the 
error has settled down. Using $B = 100$ is a good starting place. 

Ok, let's try bagging with classification trees. Here, we
will use the `train()` function with the 
`method = "treebag"` argument from the `caret` package. 

**Note**: How did I know what method to pick? 

Use help file `?train` or 
[look on caret page](http://topepo.github.io/caret/train-models-by-tag.html)
or use this: 

```{r}
names(getModelInfo())
```


```{r}
fitControl <- trainControl(method = "cv", number = 5)

treebagfit <- train(arr_delay_status ~ ., 
                    method = "treebag", data = train_dat, 
                    trControl = fitControl)
```


### Variable Importance Measures

Bagging typically results in improved accuracy over prediction
using a single tree. Unfortunately, however, it can be difficult
to interpret the resulting model. Recall that one of the advantages 
of decision trees is the attractive and easily interpreted 
diagram that results. However, when we bag a large number of
trees, it is no longer possible to represent the resulting 
statistical learning procedure using a single tree, and 
it is no longer clear which variables are most important 
to the procedure. Thus, bagging improves prediction accuracy 
at the expense of interpretability.

Although the collection of bagged trees is much more difficult
to interpret than a single tree, one can obtain an overall 
summary of the importance of each predictor using the RSS 
(for bagging regression trees) or the Gini index (for 
bagging classification trees). 

In the case of bagging regression trees, we can record the total 
amount that the RSS is decreased due to splits over a 
given predictor, averaged over all $B$ trees. A large value
indicates an important predictor. Similarly, in the context 
of bagging classification trees, we can add up the total 
amount that the Gini index is decreased by splits over a 
given predictor, averaged over all $B$ trees.


These are known as _variable importances_. 

For example, consider a set of predictors:

```{r, echo=FALSE}
knitr::include_graphics("https://topepo.github.io/caret/varimp/varImp_gbm_plot-1.svg")
```

The x-axis is "Importance of predictors" calculated as 
e.g.  total amount that the RSS is decreased due to splits over a 
given predictor, averaged over all $B$ trees. 

You can read about them in [Chapter 15](https://topepo.github.io/caret/variable-importance.html)
and see an example. 

## Random Forests

Random forests provide an improvement over bagged trees 
by way of a small tweak that _decorrelates_ the trees. 
As in bagging, we build a number of decision trees on 
bootstrapped training samples. But when building these 
decision trees, each time a split in a tree is considered,
a random sample of $m$ predictors is chosen as split
candidates from the full set of $p$ predictors. The split 
is allowed to use only one of those $m$ predictors. 

A fresh sample of $m$ predictors is taken at each split, 
and typically we choose $m \approx \sqrt{p}$, that is, 
the number of predictors considered at each split is 
approximately equal to the square root of the total number
of predictors.

In other words, in building a random forest, at each
split in the tree, the algorithm is not even allowed to 
consider a majority of the available predictors. This may 
sound crazy, but it has a clever rationale. Suppose that 
there is **one very strong predictor** in the data set, 
along with a number of other moderately strong predictors. 
Then in the collection of bagged trees, most or all of the
trees will use this strong predictor in the top split. 
Consequently, all of the bagged trees will look quite similar 
to each other. Hence the predictions from the bagged trees 
will be highly correlated. 

Unfortunately, averaging many highly correlated quantities
does not lead to as large of a reduction in variance as 
averaging many uncorrelated quantities. In particular, 
this means that bagging will not lead to a substantial 
reduction in variance over a single tree in this setting.

Random forests overcome this problem by forcing each split 
to consider only a subset of the predictors. Therefore, on 
average $(p − m)/p$ of the splits will not even consider 
the strong predictor, and so other predictors will have 
more of a chance. We can think of this process as decorrelating
the trees, thereby making the average of the resulting 
trees less variable and hence more reliable.

Here we will use the `train()` function with the 
`method = "rf"` argument from the `caret` package. 

```{r}
fitControl <- trainControl(method = "cv", number = 5)

rffit <- train(arr_delay_status ~ ., 
               method = "rf", data = train_dat, 
               trControl = fitControl)
```


```{r}
# summarize results
bagging_results <- resamples(list(# rpart = rpartfit, glm = glmfit,
                                  treebag=treebagfit, rf=rffit))
summary(bagging_results)
```

```{r}
dotplot(bagging_results)
```





### Relationship between bagging and random forests

If a random forest is built using $m = p$, then this amounts
simply to bagging. 

## Boosting 

_Boosting_ is another approach for improving the predictions 
resulting from a decision tree. Instead of _bagging_ (or building 
a tree on a bootstrap data set, independent of the other trees), 
boosting grows the trees sequentially: each tree is grown using 
information from previously grown trees. Boosting does not 
involve bootstrap sampling; instead each tree is fit on a modified
version of the original data set.

To read about the algorithmic details of boosting, check out
[Algorithm 8.2: Boosting for Regression Trees](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). 

We won't go into the details, but this is the main idea: 

Unlike fitting a single large decision tree to the data, 
which amounts to fitting the data hard and potentially
overfitting, the boosting approach instead learns slowly. 

Given the current model, we fit a decision tree to the 
residuals from the model. That is, we fit a tree using 
the current residuals, rather than the outcome $Y$, as the 
response. We then add this new decision tree into the fitted 
function in order to update the residuals. 

The idea is we are slowly improve $\hat{f}$ in areas where 
it does not perform well. In general, statistical learning 
approaches that learn slowly tend to perform well. 
Note that in boosting, unlike in bagging, the construction 
of each tree depends strongly on the trees that have already been grown.

Here we use the `method=gbm` argument for the which uses the 
[gbm](https://cran.r-project.org/web/packages/gbm/index.html) R 
package for Generalized Boosted Regression Models

```{r, message=FALSE, warning=FALSE}
fitControl <- trainControl(method = "cv", number = 5)

boostfit <- train(arr_delay_status ~ .,
                  method = "gbm", data = train_dat, 
                  trControl = fitControl, verbose = FALSE)
```


```{r}
# summarize results
summarize_results <- resamples(list(treebag = treebagfit, rf = rffit, 
                                  boost = boostfit))
summary(summarize_results)
```

```{r}
dotplot(summarize_results)
```

For more information on the [caret](http://topepo.github.io/caret/index.html)
package, you can read through the nice documention to see what other
algorithms are available for decision trees. 


## Checking test error rate 

```{r}
newdata <- as.data.frame(select(test_dat, -arr_delay_status))
pred_boost_test <- predict(boostfit, newdata)
confusionMatrix(test_dat$arr_delay_status, pred_boost_test)
```

 
